{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Humanistic Research Question...</h1>\n",
    "<img src='Wilkens 807, Table 1.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Geographic Imagination\n",
    "<ul><li>Tokenize</li>\n",
    "<ul><li>Functions</li>\n",
    "<li>For-Loops</li></ul>\n",
    "<li>Part of Speech Tags</li>\n",
    "<ul><li>Conditional Statements</li></ul>\n",
    "<li>Named Entity Recognition</li>\n",
    "<li>Geographic Imagination</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has many basic, out-of-the-box functions that we use on all the time for programming. When we want to extend our reach beyond the basics, new functions are made available through <i>packages</i> like NLTK. \n",
    "\n",
    "Packages typically need to be downloaded individually. However if you are using a platform like Anaconda (https://www.continuum.io/downloads), then many common packages are already on your computer.\n",
    "\n",
    "In order to access the new functions contained within a package, we have to <i>import</i> it into our programming environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up the Natural Language Toolkit\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that NLTK has access to appropriate models for our project\n",
    "\n",
    "modules = [\"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"punkt\"]\n",
    "\n",
    "for module in modules:\n",
    "    nltk.download(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use the opening paragraph of Pride and Prejudice throughout for our exercises\n",
    "\n",
    "paragraph = 'It is a truth universally acknowledged, that a single man \\\n",
    "in possession of a good fortune, must be in want of a wife.\\\n",
    "However little known the feelings or views of such a man may be \\\n",
    "on his first entering a neighbourhood, this truth is so well fixed \\\n",
    "in the minds of the surrounding families, that he is considered as \\\n",
    "the rightful property of some one or other of their daughters.\\\n",
    "\"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard \\\n",
    "that Netherfield Park is let at last?\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing is the field and set of methods dedicated to converting human language into something that the computer can read. It's important to keep in mind that a computer does not even know what a <i>word</i> is without receiving direct instructions from a human.\n",
    "\n",
    "Fortunately NLTK has an easy-to-implement set of instructions encoded in its function <i>word_tokenize()</i>. The idea with this function is that we can put a string of human-language text in between its parentheses and it will return a list of the individual words from that text. NLTK has a similar function, as well, called <i>sent_tokenize()</i> that does the same thing, but returns a list of individual sentences.\n",
    "\n",
    "Very often we want to tokenize our texts by word, while retaining infomation about the boundaries between sentences. In order to do this, we will first use <i>sent_tokenize()</i> and then iterate through our list of sentences with <i>word_tokenize()</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the functions we will use directly\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"What... is the air-speed velocity of an unladen swallow?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign our sentence to a variable\n",
    "\n",
    "velocity = \"What... is the air-speed velocity of an unladen swallow?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect our new variable\n",
    "\n",
    "velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feed our new variable into the function\n",
    "\n",
    "word_tokenize(velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can also assign the output of a function to a variable\n",
    "\n",
    "velocity_list = word_tokenize(velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the output variable\n",
    "\n",
    "velocity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And we can input that new variable into other functions and so on\n",
    "\n",
    "len(velocity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign three sentences of dialogue to a new variable\n",
    "\n",
    "three_sentences = \"What... is the air-speed velocity of an unladen swallow? What do you mean? An African or European swallow?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the newest variable\n",
    "\n",
    "three_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize text by word\n",
    "\n",
    "word_tokenize(three_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize text by sentence\n",
    "\n",
    "sent_tokenize(three_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Use the function word_tokenize() in order to get a list of words\n",
    "##     from the paragraph above. How many tokens does the paragraph contain?\n",
    "\n",
    "## EX. Use the function sent_tokenize() in order to get a list of sentences\n",
    "##     from the paragraph below.\n",
    "\n",
    "## Bonus: What is the average number of words per sentence in the paragraph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For-Loops\n",
    "\n",
    "We iterate through the elements in a list using the \"for\" and \"in\" syntax. You can tell those words do something special because they appear in green!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine sentence- and word-level tokenization\n",
    "\n",
    "# The line below gets indented, so that our script knows what to do\n",
    "# to each element in the list when it comes up\n",
    "\n",
    "sentence_list = sent_tokenize(three_sentences)\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternate format: List Comprehension\n",
    "\n",
    "# Collects the output from our for-loop into a new list!\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the new list\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. For the 'paragraph' from earlier, use a for-loop to get\n",
    "##     a list of words from each sentence individually.\n",
    "\n",
    "## EX. Rewrite the for-loop as a list comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour: Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those not-yet-familiar with Natural Language Processing, it often comes as a surprise how powerful word frequencies are. Simply creating a list of the unique words in a text and tallying the number of times it appears encodes information about authorship, genre, time period and author nationality among other features. Frankly, this is mind boggling!\n",
    "\n",
    "It is exceptionally easy to create this kind of tally in Python. There is a simple out-of-the-box function that we can use to count the number of times a token appears in a list. Yesterday, we looked at a function from NLTK called <i>FreqDist</i> that is a special version of the one we will look at today, <i>Counter</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import a handy counting function\n",
    "# Reports number of time each unique element appears in a list\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of tokens\n",
    "not_a_briton = \"I didn't know we had a king; I thought we were an autonomous collective.\"\n",
    "not_british_tokens = word_tokenize(not_a_briton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect token list\n",
    "not_british_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tally the appearances of each unique token\n",
    "Counter(not_british_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign the tally to a new variable\n",
    "tokens_counted = Counter(not_british_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return unique tokens, sorted by number of appearances in list\n",
    "tokens_counted.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. What is the most common word in the 'paragraph' from earlier?\n",
    "##     How often does 'truth' appear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As trained readers, we know that language partly operates according to (or sometimes against!) abstract, underlying structures, such as grammar. Identifying a word's part of speech, or tagging it, is an extremely sophisticated task that remains an open problem in the Natural Language Processing world. At this point, state-of-the-art taggers have somewhere in the neighborhood of 98% accuracy.\n",
    "\n",
    "NLTK's default tagger, <i>pos_tag()</i>, has an accuracy just shy of that with the trade-off that it is comparatively fast. Simply place a list of tokens between its parentheses and it returns a new list where each item is the original word alongside its predicted part of speech.\n",
    "\n",
    "The tags themselves come from the Penn Treebank and a full list of them can be found here: <a href=\"http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Common POS taggers\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.perceptron</td>\n",
    "        <td>import PerceptronTagger</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.brill</td>\n",
    "        <td>import BrillTagger</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.stanford</td>\n",
    "        <td>import StanfordTagger, StanfordPOSTagger, StanfordNERTagger</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: NLTK simply offers a wrapper for the Stanford taggers, which allows you to use them in Python, rather than their native Java. Stanford models must be downloaded from here: http://nlp.stanford.edu/software/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK's current default POS tagger is the 'averaged perceptron' as described here:\n",
    "# https://spacy.io/blog/part-of-speech-POS-tagger-in-python\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Create variable for new sentence\n",
    "new_sentence = \"Once the number three, being the third number, be reached, \\\n",
    "then lobbest thou thy Holy Hand Grenade of Antioch towards thy foe, \\\n",
    "who, being naughty in My sight, shall snuff it.\"\n",
    "\n",
    "# Create list of word tokens\n",
    "new_tokens = word_tokenize(new_sentence)\n",
    "\n",
    "# Assign a POS tag to each token\n",
    "pos_tag(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's refresh ourselves on the functions and for-loops from earlier\n",
    "\n",
    "# An old variable revisited!\n",
    "three_sentences = \"What... is the air-speed velocity of an unladen swallow? What do you mean? An African or European swallow?\"\n",
    "\n",
    "# Re-make the list of sentences from the text\n",
    "sentence_list = sent_tokenize(three_sentences)\n",
    "\n",
    "# Re-tokenize each sentence by word\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the list of lists of tokens\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now iterate through the tokenized sentences and POS tag them\n",
    "for sentence in tokenized_sentences:\n",
    "    print(pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the tagged sentences in a list\n",
    "tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Get POS tags for the very new sentence below.\n",
    "\n",
    "## EX. Get POS tags for the 'paragraph' from Pride and Prejudice.\n",
    "##     Collect these tags using a list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "very_new_sentence = \"On second thought, let's not go to Camelot.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The entries in each tagged sentence consist of a token-tag pair.\n",
    "# Sometimes we just want one of those values.\n",
    "\n",
    "# When the entries in a list are paired like the (token,tag) format above,\n",
    "# we can label the elements seperately while we iterate through\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Of course, we can access either value in the pair\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also add a condition: IF the condition is TRUE,\n",
    "# then the script continues with the next indented line.\n",
    "# Otherwise, it gets skipped!\n",
    "\n",
    "# Calling the noun tag for our IF statement\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        if tag=='NN':\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calling the adjective tag for our IF statement\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        if tag=='JJ':\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The double equals sign is a test of equality NOT a variable assignment\n",
    "\n",
    "5 == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Return the nouns from the opening paragraph of Pride and Prejudice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among parts of speech, names and proper nouns are of particular significance, since they are the more-or-less unique keywords that identify phenomena of social relevance (including people, places, and institutions). After all, there is just one <i>World War II</i>, and in a novel, a name like <i>Mr. Darcy</i> typically acts as a more-or-less stable referent over the course of the text. (Or perhaps we are interested in thinking about the degree of instability with which it is used!)\n",
    "\n",
    "The identification of these kinds of names is referred to as Named Entity Recognition, or NER. The challenge is twofold. First, it has to be determined whether a name spans multiple tokens. (These multi-token grammatical units are referred to as <i>chunks</i>; the process, <i>chunking</i>.) Second, we would ideally distinguish among categories of entity. Is <i>Mr. Darcy</i> a geographic location? Just who is this <i>World War II</i> I hear so much about?\n",
    "\n",
    "To this end, the function ne_chunk() receives a list of tokens including their parts of speech and returns a nested list where named entities' tokens are chunked together, along with their category as predicted by the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's start with a fresh sentence containing several proper names\n",
    "\n",
    "ner_sentence = 'King Arthur is the sovereign over Britain and lord of the Round Table.'\n",
    "ner_tokens = word_tokenize(ner_sentence)\n",
    "ner_tags = pos_tag(ner_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the POS tags\n",
    "ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the NER funtion\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "chunks = ne_chunk(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK is finicky here, so we need to use 'print' to inspect\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll iterate through our list of chunks. Name Entities are grouped\n",
    "# together into 'nltk.tree.Tree'. (This is an under-the-hood data type.)\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "            print(chunk.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's select just ones with the 'GPE' (Geo-Political Entity) designation\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "        if chunk.label()=='GPE':\n",
    "            print(chunk.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# When we have multiple conditions -- i.e. multiple 'if' statements --\n",
    "# we can put them together on a line using 'and'.\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree and chunk.label()=='GPE':\n",
    "            print(chunk.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rewrite it as a list comprehension!\n",
    "# Note that the 'if' statement goes *after* the 'for'-'in syntax\n",
    "\n",
    "gpe_chunks = [chunk.leaves() for chunk in chunks if type(chunk)==nltk.tree.Tree and chunk.label()=='GPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the new list\n",
    "\n",
    "gpe_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We ultimately don't need the POS tag along with the place\n",
    "# name, so we can iterate through and pull names out\n",
    "\n",
    "for gpe in gpe_chunks:\n",
    "    for name,tag in gpe:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now as a list comprehension!\n",
    "\n",
    "names_only = [name for gpe in gpe_chunks for name,tag in gpe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the newest list\n",
    "\n",
    "names_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. Retrieve the place names (excluding POS tags) from the sentence below.\n",
    "\n",
    "## EX. Rewrite the previous exercise as a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swallow_skeptic = \"Oh yeah, an African swallow, maybe, but not a European swallow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Geographic Imagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to study the changing attention of American literature during the nineteenth century, Matthew Wilkens counts the frequencies of place names and compares them before and after the Civil War. For now, we will limit our study to a single text, Kate Chopin's <i>The Awakening</i>. Using the techniques from this lesson, count the number of times each place name appears in this text. Return a list of the most common place names.\n",
    "\n",
    "\n",
    "Q. Does the list of the most common names make sense? Are there things that don't? How might you change the program to handle names differently?\n",
    "\n",
    "<i>Note: Wilkens includes notes on his own NER process on pp 833-835.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read text of The Awakening from file\n",
    "# Creates variable 'chopin_text' with whole text in single string\n",
    "\n",
    "chopin_text = open('Chopin - The Awakening & Selected Short Stories.txt').read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

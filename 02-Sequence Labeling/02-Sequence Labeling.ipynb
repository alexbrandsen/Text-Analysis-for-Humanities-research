{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sequence Labeling\n",
    "<ul><li>Tokenize</li>\n",
    "<ul><li>Functions</li>\n",
    "<li>For-Loops</li></ul>\n",
    "<li>Part of Speech Tags</li>\n",
    "<ul><li>Conditional Statements</li></ul>\n",
    "<li>Named Entity Recognition</li>\n",
    "<li>Geographic Imagination</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has many basic, out-of-the-box functions that we use all the time for programming. When we want to extend our reach beyond the basics, new functions are made available through <i>packages</i> like NLTK. \n",
    "\n",
    "Packages typically need to be downloaded individually. However if you are using a platform like Anaconda (https://www.continuum.io/downloads), then many common packages are already on your computer.\n",
    "\n",
    "In order to access the new functions contained within a package, we have to <i>import</i> it into our programming environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install modules (using pip)\n",
    "!pip install nltk\n",
    "!pip install numpy\n",
    "\n",
    "# Import modules\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that NLTK has access to appropriate models for our project\n",
    "\n",
    "modules = [\"averaged_perceptron_tagger\", \"maxent_ne_chunker\", \"punkt\", \"words\"]\n",
    "\n",
    "for module in modules:\n",
    "    nltk.download(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the opening paragraph of Renfrew & Bahn's \"Archaeology: Theories, Methods and Practice\" \n",
    "# throughout this notebook for our exercises\n",
    "\n",
    "paragraph = 'Archaeology is partly the discovery of the treasures of the \\\n",
    "past, partly the work of the scientific analyst, partly the \\\n",
    "exercise of the creative imagination. It is toiling in the sun \\\n",
    "on an excavation in the deserts of Central Asia, it is working \\\n",
    "with living Inuit in the snows of Alaska. It is diving down to \\\n",
    "Spanish wrecks off the coast of Florida, and it is investigating  \\\n",
    "the sewers of Roman York. But it is also the analysis \\\n",
    "of materials in the laboratory, and the interpretation of \\\n",
    "what these things mean for the human story. Finally, \\\n",
    "archaeology is also the conservation of the worldâ€™s cultural \\\n",
    "heritage, the understanding of stakeholders of the past, \\\n",
    "and the protection of the past from looting and careless destruction' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing is the field and set of methods dedicated to converting human language into something that the computer can read. It's important to keep in mind that a computer does not even know what a <i>word</i> is without receiving direct instructions from a human.\n",
    "\n",
    "Fortunately NLTK has an easy-to-implement set of instructions encoded in its function <i>word_tokenize()</i>. The idea with this function is that we can put a string of human-language text in between its parentheses and it will return a list of the individual words from that text. NLTK has a similar function, as well, called <i>sent_tokenize()</i> that does the same thing, but returns a list of individual sentences.\n",
    "\n",
    "Very often we want to tokenize our texts by word, while retaining infomation about the boundaries between sentences. In order to do this, we will first use <i>sent_tokenize()</i> and then iterate through our list of sentences with <i>word_tokenize()</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions we will use directly\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"What is the purpose of Stonehenge?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign our sentence to a variable\n",
    "\n",
    "ylvis = \"What is the purpose of Stonehenge?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect our new variable\n",
    "\n",
    "ylvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed our new variable into the function\n",
    "\n",
    "word_tokenize(ylvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also assign the output of a function to a variable\n",
    "\n",
    "ylvis_list = word_tokenize(ylvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the output variable\n",
    "\n",
    "ylvis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can input that new variable into other functions and so on\n",
    "\n",
    "len(ylvis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign three sentences of dialogue to a new variable\n",
    "\n",
    "three_sentences = \"What is the purpose of Stonehenge? A giant granite birthday cake? Or a prison far too easy to escape?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the newest variable\n",
    "\n",
    "three_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text by word\n",
    "\n",
    "word_tokenize(three_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text by sentence\n",
    "\n",
    "sent_tokenize(three_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Use the function word_tokenize() in order to get a list of words\n",
    "##           from the paragraph from Renfrew & Bahn at the top of this notebook. \n",
    "##           How many tokens does the paragraph contain?\n",
    "\n",
    "## EXERCISE. Use the function sent_tokenize() in order to get a list of sentences\n",
    "##           from the Renfrew & Bahn paragraph.\n",
    "\n",
    "## Bonus: What is the average number of words per sentence in this paragraph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For-Loops\n",
    "\n",
    "We iterate through the elements in a list using the \"for\" and \"in\" syntax. You can tell those words do something special because they appear in green!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentence- and word-level tokenization\n",
    "\n",
    "# The line below gets indented, so that our script knows what to do\n",
    "# to each element in the list when it comes up\n",
    "\n",
    "sentence_list = sent_tokenize(three_sentences) # save sentences in variable\n",
    "\n",
    "for sentence in sentence_list: # tell Python to go through all sentences in the sentence_list and..\n",
    "    print(word_tokenize(sentence)) # ... print the list of tokens in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. For the paragraph from Renfrew & Bahn from earlier, use a for-loop to get\n",
    "##           a list of words from each sentence individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour: Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those not-yet-familiar with Natural Language Processing, it often comes as a surprise how powerful word frequencies are. Simply creating a list of the unique words in a text and tallying the number of times it appears encodes information about authorship, genre, time period and author nationality among other features. Frankly, this is mind boggling!\n",
    "\n",
    "It is exceptionally easy to create this kind of tally in Python. There is a simple out-of-the-box function that we can use to count the number of times a token appears in a list. Yesterday, we looked at a function from NLTK called <i>FreqDist</i> that is a special version of the one we will look at today, <i>Counter</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a handy counting function\n",
    "# Reports number of time each unique element appears in a list\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tokens\n",
    "indiana_jones_quote = \"If you want to be a good archeologist, you have got to get out of the library!\"\n",
    "jones_tokens = word_tokenize(indiana_jones_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect token list\n",
    "jones_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally the appearances of each unique token\n",
    "Counter(jones_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the tally to a new variable\n",
    "tokens_counted = Counter(jones_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return unique tokens, sorted by number of appearances in list\n",
    "tokens_counted.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. What is the most common word in the Renfrew & Bahn paragraph from earlier?\n",
    "##           How often does 'past' appear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As trained readers, we know that language partly operates according to (or sometimes against!) abstract, underlying structures, such as grammar. Identifying a word's part of speech, or tagging it, is an extremely sophisticated task that remains an open problem in the Natural Language Processing world. At this point, state-of-the-art taggers have somewhere in the neighborhood of 98% accuracy.\n",
    "\n",
    "NLTK's default tagger, <i>pos_tag()</i>, has an accuracy just shy of that with the trade-off that it is comparatively fast. Simply place a list of tokens between its parentheses and it returns a new list where each item is the original word alongside its predicted part of speech.\n",
    "\n",
    "The tags themselves come from the Penn Treebank and a full list of them can be found here: <a href=\"http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\">www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common POS taggers\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.perceptron</td>\n",
    "        <td>import PerceptronTagger</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.brill</td>\n",
    "        <td>import BrillTagger</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>from nltk.tag.stanford</td>\n",
    "        <td>import StanfordTagger, StanfordPOSTagger, StanfordNERTagger</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: NLTK simply offers a wrapper for the Stanford taggers, which allows you to use them in Python, rather than their native Java. Stanford models can be downloaded from here: http://nlp.stanford.edu/software/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's current default POS tagger is the 'averaged perceptron' as described here:\n",
    "# https://spacy.io/blog/part-of-speech-POS-tagger-in-python\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Create variable for new sentence\n",
    "new_sentence = \"Broadly, archaeology is the study of the human past through its material remains.\"\n",
    "\n",
    "# Create list of word tokens\n",
    "new_tokens = word_tokenize(new_sentence)\n",
    "\n",
    "# Assign a POS tag to each token\n",
    "pos_tag(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's refresh ourselves on the functions and for-loops from earlier\n",
    "\n",
    "# An old variable revisited!\n",
    "three_sentences = \"What is the purpose of Stonehenge? A giant granite birthday cake? Or a prison far too easy to escape?\"\n",
    "\n",
    "# Re-make the list of sentences from the text\n",
    "sentence_list = sent_tokenize(three_sentences)\n",
    "\n",
    "# Re-tokenize each sentence by word\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the list of lists of tokens\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now iterate through the tokenized sentences and POS tag them\n",
    "for sentence in tokenized_sentences:\n",
    "    print(pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the tagged sentences in a list\n",
    "tagged_sentences = [pos_tag(sentence) for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Get POS tags for the very new sentence below.\n",
    "\n",
    "very_new_sentence = \"We do not follow maps to buried treasure, and X never, ever marks the spot.\"\n",
    "\n",
    "## EXERCISE. Get POS tags for the paragraph from Renfrew and Bahn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entries in each tagged sentence consist of a token-tag pair.\n",
    "# Sometimes we just want one of those values.\n",
    "\n",
    "# When the entries in a list are paired like the (token,tag) format above,\n",
    "# we can label the elements seperately while we iterate through\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, we can access either value in the pair\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also add a condition: IF the condition is TRUE,\n",
    "# then the script continues with the next indented line.\n",
    "# Otherwise, it gets skipped!\n",
    "\n",
    "# Calling the noun tag for our IF statement\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        if tag=='NN':\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the adjective tag for our IF statement\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    for token, tag in sentence:\n",
    "        if tag=='JJ':\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The double equals sign is a test of equality NOT a variable assignment\n",
    "\n",
    "5 == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Return the nouns from the opening paragraph of Renfrew & Bahn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among parts of speech, names and proper nouns are of particular significance, since they are the more-or-less unique keywords that identify phenomena of social relevance (including people, places, time periods, artefacts, etc). After all, there is just one <i>Germany</i>, and in an excavation report, a word like <i>Neolithic</i> typically acts as a more-or-less stable referent over the course of the text. (Or perhaps we are interested in thinking about the degree of instability with which it is used!)\n",
    "\n",
    "The identification of these kinds of names is referred to as Named Entity Recognition, or NER. The challenge is twofold. First, it has to be determined whether a name spans multiple tokens. (These multi-token grammatical units are referred to as <i>chunks</i>; the process, <i>chunking</i>.) Second, we would ideally distinguish among categories of entity. Is <i>Neolithic</i> a geographic location? Just who is this <i>Germany</i> I hear so much about?\n",
    "\n",
    "To this end, the function ne_chunk() receives a list of tokens including their parts of speech and returns a nested list where named entities' tokens are chunked together, along with their category as predicted by the computer.\n",
    "\n",
    "Unfortunately, standard NER methods (such as this one), only find standard named entities, such as persons, locations and organisations, and not the entities we are insterested in as archaeologists (artefacts, species, etc). Standard named entities are used here as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a fresh sentence containing several proper names\n",
    "\n",
    "ner_sentence = 'King Arthur is the sovereign over Britain and lord of the Round Table.'\n",
    "ner_tokens = word_tokenize(ner_sentence)\n",
    "pos_tags = pos_tag(ner_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the POS tags\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the NER funtion\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "chunks = ne_chunk(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK is finicky here, so we need to use 'print' to inspect\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll iterate through our list of chunks. Name Entities are grouped\n",
    "# together into 'nltk.tree.Tree'. (This is an under-the-hood data type.)\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's select just ones with the 'GPE' (Geo-Political Entity) designation, and only print the entity\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree:\n",
    "        if chunk.label()=='GPE':\n",
    "            print(\" \".join([token[0] for token in chunk.leaves()])) # this just loops over the tokens in the entity, and joins them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we have multiple conditions -- i.e. multiple 'if' statements --\n",
    "# we can put them together on a line using 'and'.\n",
    "\n",
    "for chunk in chunks:\n",
    "    if type(chunk)==nltk.tree.Tree and chunk.label()=='GPE':\n",
    "            print(\" \".join([token[0] for token in chunk.leaves()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE. Retrieve the place names (excluding POS tags) from the sentence below.\n",
    "\n",
    "swallow_skeptic = \"Oh yeah, an African swallow, maybe, but not a European swallow.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Optional Assignment: Geography in Renfrew & Bahn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excerpt of a chapter of Renfrew & Bahn has been added in the folder. This bit of text describes how to survey and excavate, using examples from around the world.\n",
    "\n",
    "Using the techniques from this lesson, count the number of times each place name appears in this text. Return a list of the most common place names.\n",
    "\n",
    "\n",
    "Questions:\n",
    "\n",
    "- Does the list of the most common names make sense? \n",
    "- Are there things that don't? \n",
    "- Do you see any errors?\n",
    "- Do you think there is any geographical bias in the examples used by Renfrew & Bahn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample text of Renfrew & Bahn chapter from file\n",
    "# Creates variable 'rb_text' with whole text in single string\n",
    "\n",
    "rb_text = open('r_and_b_sample.txt',encoding=\"utf-8\").read()\n",
    "\n",
    "\n",
    "# Hint: you will need to create a new, empty list with \"list = []\", and then use \"list.append(item)\" to add all place names to this list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution can be found in the file \"r_and_b_ner_solution.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with the Very Idea of a Humanistic Research Question...</h1>\n",
    "<img src='Moretti 7, Fig 7.png' width=\"66%\" height=\"66%\">\n",
    "<br>\n",
    "<img src='Moretti 11, excerpt.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operationalizing\n",
    "\n",
    "<ul>\n",
    "<li>New Methods</li>\n",
    "<ul>\n",
    "<li>String</li>\n",
    "<li>Dictionary</li>\n",
    "</ul>\n",
    "<li>Import Corpus</li>\n",
    "<li>Pre-Process Corpus</li>\n",
    "<li>Pandas</li>\n",
    "<li>Statistics</li>\n",
    "<ul>\n",
    "<li>Character Space</li>\n",
    "<li>Most Distinctive Words</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Review/Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the texts of a set of novels, using yesterday's method\n",
    "\n",
    "import os\n",
    "novel_corpus_path = 'txtalb_Novel150_English/'\n",
    "novel_file_names = os.listdir(novel_corpus_path)\n",
    "novel_texts = [open(novel_corpus_path+file_name).read() for file_name in novel_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a peek at the first novel\n",
    "\n",
    "novel_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize, remove stop words, remove low frequency tokens, featurize as True-False\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', min_df = 2, binary=True)\n",
    "\n",
    "novel_dtm = cv.fit_transform(novel_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use 'pandas' package to get the output into human-readable format\n",
    "\n",
    "import pandas\n",
    "\n",
    "pandas.DataFrame(novel_dtm.toarray(), columns = cv.get_feature_names(), index=novel_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extending our Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings\n",
    "\n",
    "Strings and string methods are been our bread and butter throughout the workshop. We have already seen them assigned to variables, split over white spaces, added together, and sliced by index. Let's review those techniques and try out a couple variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's assign a string to a new variable\n",
    "# Using the triple quotation mark, we can simply paste a passage in between\n",
    "# and Python will treat it as a continuous string\n",
    "\n",
    "first_sonnet = \"\"\"From fairest creatures we desire increase,\n",
    "That thereby beauty's rose might never die,\n",
    "But as the riper should by time decease,\n",
    "His tender heir might bear his memory\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that when we print the 'first_sonnet', we see the character\n",
    "# that represents a line break: '\\n'\n",
    "\n",
    "first_sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A familiar string method\n",
    "\n",
    "first_sonnet.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With a twist!\n",
    "\n",
    "first_sonnet.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In fact, we can split over any character\n",
    "\n",
    "first_sonnet.split('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Of course, we can find the length of a string in characters\n",
    "\n",
    "len(first_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And even slice it by character position!\n",
    "\n",
    "first_sonnet[13:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also reverse engineer the location of string patterns\n",
    "\n",
    "first_sonnet.index('creatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This method only returns the first instance of the string pattern\n",
    "\n",
    "first_sonnet.index('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A handy trick\n",
    "\n",
    "first_sonnet.index('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's assign another string to a variable, shall we?\n",
    "\n",
    "second_quatrain = \"\"\"But thou, contracted to thine own bright eyes,\n",
    "Feed'st thy light's flame with self-substantial fuel,\n",
    "Making a famine where abundance lies, \n",
    "Thyself thy foe, to thy sweet self too cruel.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember that we can add strings together\n",
    "\n",
    "first_sonnet + second_quatrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can also assign them back to one of the variables!\n",
    "\n",
    "first_sonnet = first_sonnet + \"\\n\" + second_quatrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Et voila!\n",
    "\n",
    "first_sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. How long is the first word of each line in 'first_sonnet'?\n",
    "##     Hint: The first word ends at the first space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Although we used dictionaries extensively yesterday in our applications, we didn't talk about them formally. Let's rectify that!\n",
    "\n",
    "A <i>dictionary</i> is a data type in Python that is used to contain or organize other pieces of data. This is similar to the <i>list</i> data type that we have used extensively and that contains numbers, strings, and even other lists. Whereas lists organize data by keeping track of their order, a dictionary organizes data by means of a labeling system.\n",
    "\n",
    "In a real-word dictionary, an entry can be found by its word-label and within the entry is a definition. In a Python dictionary, entries are labeled with <i>keys</i> and they contain <i>values</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Yesterday's first dictionary\n",
    "# Keys were tokens from the sentence; Values were a True value representing its presence\n",
    "\n",
    "{'high': True, 'air-speed': True, 'velocity': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to a variable\n",
    "\n",
    "old_dictionary = {'high': True, 'air-speed': True, 'velocity': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call up the value for a given key\n",
    "\n",
    "old_dictionary['high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What about a key that isn't in the dictionary\n",
    "\n",
    "old_dictionary['low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's add it to the dictionary by giving it a value\n",
    "\n",
    "old_dictionary['low'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "old_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of all keys\n",
    "\n",
    "old_dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a list of all values (same order as the key list!)\n",
    "\n",
    "old_dictionary.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new, empty dictionary\n",
    "\n",
    "new_dictionary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add an entry\n",
    "# In this case, both key and entry are strings!\n",
    "\n",
    "new_dictionary['breakfast'] = 'egg and spam; egg bacon and spam; egg bacon sausage and spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Create a dictionary in which each key is a unique word from this\n",
    "##     famous line, along with the value 'True'\n",
    "\n",
    "## EX. Create a new dictionary in which each key is a unique word from this\n",
    "##     famous line and each value is the number of letters in the word\n",
    "\n",
    "famous_line = 'To be or not to be that is the question'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Corpus\n",
    "\n",
    "Moretti had performed his study of <i>Antigone</i> by collecting and dividing the speech belonging to each character. There are many ways to do this, but one elegant way is to create a dictionary, in which each entry belongs to a unique character. A key will be a name and a value will be a string with all of the words uttered by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the text of Antigone from a file on your hard drive\n",
    "\n",
    "antigone_text = open('antigone.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "antigone_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list by splitting the string whereever a double line break occurs\n",
    "\n",
    "antigone_list = antigone_text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "antigone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First line\n",
    "\n",
    "antigone_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's assign it to a variable and get a feel for an important property\n",
    "\n",
    "first_line = antigone_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the first space\n",
    "\n",
    "first_line.index(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slice the line before that space\n",
    "\n",
    "first_line[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slice the line after that space\n",
    "\n",
    "first_line[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new, empty dictionary\n",
    "\n",
    "dialogue_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember the for-loop with conditional statements?\n",
    "\n",
    "# Iterate through each of the play's lines\n",
    "for line in antigone_list:\n",
    "    \n",
    "    # Find the first space in each line\n",
    "    index_first_space = line.index(' ')\n",
    "    \n",
    "    # Slice the line, preceding the first space\n",
    "    character_name = line[:index_first_space]\n",
    "    \n",
    "    # Check whether the character is in our dictionary yet\n",
    "    if character_name not in dialogue_dict.keys():\n",
    "        \n",
    "        # If not, create a new entry whose value is a slice of the line *after* the first space\n",
    "        dialogue_dict[character_name] = line[index_first_space:]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If so, add the slice of line to the existing value\n",
    "        dialogue_dict[character_name] = dialogue_dict[character_name] + line[index_first_space:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "dialogue_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single character\n",
    "\n",
    "dialogue_dict['ANTIGONE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Choose one of the following:\n",
    "\n",
    "## EX. Create a new dictionary in which each key is the name of a character from Antigone\n",
    "##     and each value is the total number of words spoken.\n",
    "\n",
    "## EX. Create a dictionary in which each entry is the dialogue beloning to an individual character in Hamlet.\n",
    "##     Note that the text of Hamlet is formatted slightly differently from that of Antigone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hamlet_text = open('hamlet.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-Process Corpus\n",
    "\n",
    "In yesterday's lesson, pre-processing was an arduous task that took most of our effort. As it turns out, there is a popular package, <i>scikit learn</i>, containing a function that makes pre-processing very easy. We can use it to tokenize, remove stop words, select common words, and count their frequencies in a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We used the dictionary to keep our strings organized by label above,\n",
    "# but now we'll keep track of them by list order\n",
    "\n",
    "dialogue_list = dialogue_dict.values()\n",
    "character_list = dialogue_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "dialogue_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're going to remove stop words from our text, but we won't just use the basic list,\n",
    "# since our translation of 'Antigone' affects an archaic diction\n",
    "\n",
    "# Get NLTK's stopword list\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's remind ourselves what the list contains\n",
    "\n",
    "english_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a custom list of stop words\n",
    "ye_olde_stop_words = ['thou','thy','thee', 'ye', 'hath','hast', 'wilt',\\\n",
    "                      'art', 'dost','doth','shalt','tis','canst','thyself']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine these lists\n",
    "\n",
    "all_stop_words = english_stop_words + ye_olde_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the pre-processing function 'CountVectorizer'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the function to remove stop words; assign to a variable\n",
    "\n",
    "cv = CountVectorizer(stop_words=all_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-process the text of the play\n",
    "# Produces a document-term matrix\n",
    "\n",
    "dtm = cv.fit_transform(dialogue_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect a *slightly* more readable format\n",
    "\n",
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of vocabulary words from the pre-processor\n",
    "# They double as column labels!\n",
    "\n",
    "vocabulary_list = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "vocabulary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And earlier, we created a list of characters that now match our rows!\n",
    "\n",
    "character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our dataframe package!\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a human-readable document-term matrix!\n",
    "\n",
    "pandas.DataFrame(dtm.toarray(), columns = vocabulary_list, index = character_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign to a variable for later\n",
    "\n",
    "dtm_df = pandas.DataFrame(dtm.toarray(), columns = vocabulary_list, index = character_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Choose one of the following:\n",
    "\n",
    "## EX. Try initializing the CountVectorizer function so that it removes words that appear\n",
    "##     in just one document. (See example at beginning of lesson.)\n",
    "##     How many columns remain in the document-term matrix?\n",
    "\n",
    "## EX. Create a document-term matrix for Hamlet, in which each row is a character\n",
    "##     and each column a unique word. Do not include stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pandas\n",
    "\n",
    "Pandas is a popular and flexible package whose primary use is its datatype: the <i>DataFrame</i>. The dataframe is essentially a spreadsheet, like you would find in Excel, but it integrates seamlessly into a Natural Language Processing workflow and it has a few tricks up its sleeve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of three sub-lists, each with three entries\n",
    "\n",
    "square_list = [[1,2,3],[4,5,6],[7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe from that list\n",
    "\n",
    "pandas.DataFrame(square_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a couple of lists for our column and row labels\n",
    "\n",
    "column_names = ['Eggs', 'Bacon', 'Sausage']\n",
    "row_names = ['Served','With','Spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A-ha!\n",
    "\n",
    "pandas.DataFrame(square_list, columns = column_names, index=row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign this to a variable\n",
    "\n",
    "spam_df = pandas.DataFrame(square_list, columns = column_names, index=row_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call up a column of the dataframe\n",
    "\n",
    "spam_df['Eggs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make that column into a list\n",
    "\n",
    "list(spam_df['Eggs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the indices for the entries in the column\n",
    "\n",
    "spam_df['Eggs'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call up a row from the indices\n",
    "\n",
    "spam_df.loc['Served']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call up a couple of rows, using a list of indices!\n",
    "\n",
    "spam_df.loc[['Spam','Served']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a specific entry by calling both row and column\n",
    "\n",
    "spam_df.loc['Spam']['Eggs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column\n",
    "\n",
    "spam_df['Lobster Thermidor aux crevettes'] = [10,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "spam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Call up the entries (5 and 6) from the middle of the dataframe 'spam_df' individually\n",
    "\n",
    "## CHALLENGE: Call up both entries at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slice out a column\n",
    "\n",
    "spam_df['Bacon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate whether each element in the column is greater than 5\n",
    "\n",
    "spam_df['Bacon']==5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use that evaluation to subset the table\n",
    "\n",
    "spam_df[spam_df['Bacon']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Slice 'spam_df' to contain only rows in which 'Sausage' is greater than 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Statistics!\n",
    "\n",
    "A mentor of mine once joked that the only training one needs for digital literary analysis is how to construct a document-term matrix. After that you simply hand the trainee a statistics textbook.\n",
    "\n",
    "The DTM is the basis for not only Moretti's study of dramatic character but the vast majority of studies in the field. Sometimes the rows are characters in a play; sometimes they are individual poems or novels. The columns are very often the unique words contained in a corpus. If we think creatively, the text processing from each of the three previous workshops in this series can be represented in a DTM.\n",
    "\n",
    "The digital humanist's call to arms: If words are as powerful and multivalent as humanists like to believe, would we not expect to find patterns in the matrix of their frequencies? Would those patterns not have essential interpretive value? Statistics will be the method for identifying these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our document-term matrix\n",
    "\n",
    "spam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pandas will produce a few descriptive statistics for each row\n",
    "\n",
    "spam_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiply entries of the DTM by 10\n",
    "\n",
    "spam_df*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add 10 to each entry\n",
    "\n",
    "spam_df+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Of course our dataframe hasn't changed\n",
    "\n",
    "spam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also perform operations among columns\n",
    "# Pandas knows to match up individual entries in each column\n",
    "\n",
    "spam_df['Bacon']/spam_df['Eggs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Space\n",
    "\n",
    "In Moretti's study, he offers several measures of the concept of character-space. The simplest of these is to measure the relative dialogue belong to each character in a play. Presumably the main characters will speak more and peripheral characters will speak less.\n",
    "\n",
    "The statistical moves we will make here are not only counting the raw number of words spoken by each character but also normalizing them. That is, converting them into a fraction of all words in the play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a sum of each column\n",
    "\n",
    "dtm_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a sum of each row\n",
    "\n",
    "dtm_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign this to a variable\n",
    "\n",
    "raw_counts = dtm_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize!\n",
    "\n",
    "# Tells Jupyter to produce images in notebook\n",
    "% pylab inline\n",
    "\n",
    "# Makes images look good\n",
    "style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize using the 'plot' method from Pandas\n",
    "\n",
    "raw_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the total number of words\n",
    "\n",
    "sum(raw_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to variable\n",
    "\n",
    "total_counts = sum(raw_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use that total to normalize the share of words belonging to each character\n",
    "\n",
    "raw_counts/total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to a variable\n",
    "\n",
    "normed_counts = raw_counts/total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get them in order of most prominent speaker\n",
    "\n",
    "normed_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reassign variable, for convenience\n",
    "\n",
    "normed_counts = normed_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "normed_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Most Distinctive Words\n",
    "\n",
    "This is a clever technique that can be used to determine which words are most prominently associated with a single character in a text (or with a single text in a corpus), and it is a kind of house method that appears regularly in Stanford's LitLab pamphlet series.\n",
    "\n",
    "The MDW method relies two measurements: first it observes the number of times a given word is spoken by a character, and second it constructs an expected number of times the word would be spoken by the character if all things were equal. Finally, these measurements get compared as the ratio of Observed Frequency divided by Expected Frequency.\n",
    "\n",
    "<table>\n",
    "<tr><b><td>Word Spoken by Antigone</td><td>Observed</td><td>Expected</td><td>O/E Ratio</td></b></tr>\n",
    "<tr><td>brother</td><td>10</td><td>2.1</td><td>4.7</td></tr>\n",
    "</table>\n",
    "\n",
    "Expected Frequency is determined by looking at the total share of words spoken by a character and multiplying that fraction by the total number of times a given word appears in the text. This produces a kind of weighted average.\n",
    "\n",
    "For example, the word \"brother\" appears in the play 14 times. Antigone speaks about 15% of all words in the play, so we would expect that -- if she were totally average -- she would use the word about 2 times. (14 x 0.15 = 2.1) However, she says the word 10 times, or nearly five times more often than we expected. (10 / 2.1 = 4.7) Perhaps her relationship to her brother is a disctinctive characteristic! In general, a character's MDWs have an O/E ratio greater than 1.\n",
    "\n",
    "Note that this method is sensitive to low frequency words. For the sake of validity, we typically throw out words that are spoken fewer than five times by a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Produce a list of the Most Distinctive Words Spoken by Antigone.\n",
    "\n",
    "##     Using the empty dataframe below, add columns for the observed word frequencies\n",
    "##     of relevance to this problem and perform operations on these as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create new, empty dataframe\n",
    "\n",
    "mdw_df = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: Depending on how you approach the problem, you may wish to add columns for\n",
    "#       Antigone's observed word counts and the total counts of each word in the play\n",
    "\n",
    "mdw_df['ANTIGONE'] = dtm_df.loc['ANTIGONE']\n",
    "mdw_df['WORD_TOTAL'] = dtm_df.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Humanistic Research Question...</h1>\n",
    "<img src='Thornbury 170, Fig 4.5.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Python for Humanists\n",
    "\n",
    "### Motivating Project\n",
    "\n",
    "### Programming in Python\n",
    "<ul>\n",
    "    <li>Basic Data Types & Operations</li>\n",
    "    <ul>\n",
    "        <li>Arithmetic</li>\n",
    "        <li>Variable Assignment</li>\n",
    "        <li>Strings</li>\n",
    "        <li>Lists</li>\n",
    "    </ul>\n",
    "    <li>A few tricks up your sleeve</li>\n",
    "    <ul>\n",
    "        <li>String Methods</li>\n",
    "        <li>List Comprehension</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "### Introduction to NLTK\n",
    "<ul><li>Modules & Corpora</li>\n",
    "<li>Concordance Building</li>\n",
    "<li>Word Frequencies</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Building Intuition\n",
    "\n",
    "Inside the folder that contains this script, there is a plaintext file with notes that I took during Professor Emily Thornbury's talk \"Stop Having Ideas and Start Counting.\" The title contains two present progressive (<i>-ing</i>) verbs, which might suggest we look for others in the body of the text, if we were to do something like a literary analysis.\n",
    "\n",
    "Part of the reason why people use Python to do work on human-language texts (<i>natural language processing</i>) is because it makes tasks like this relatively simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (don't worry about understanding everything here)\n",
    "for line in open('lecture notes 09-22-15.txt'):\n",
    "    for word in line.split():\n",
    "        if word.endswith('ing'):\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Data Types & Operations\n",
    "## Arithmetic\n",
    "\n",
    "Before doing any more NLP, let's start with the basics. Any time you work with computers, it is essential to remember that they are simply counting machines. Place-holders with zeros and ones represent numbers that get added and subtracted from one another. This is true even for language -- but let's not get ahead of ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Addition\n",
    "\n",
    "2+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's have Python report the results from three operations at the same time\n",
    "\n",
    "print(2-5)\n",
    "print(2*5)\n",
    "print(2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we have all of our operations in the last line of the cell, Jupyter will print them together\n",
    "\n",
    "2-5, 2*5, 2/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And let's compare values\n",
    "\n",
    "2>5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable assignment\n",
    "\n",
    "Assigning variables is something that we do all the time in programming. These aren't quite like the variables from high school algebra, where <i>x</i> represents an unknown to solve for. Instead these are like notes to ourselves that we want to save some value(s) for later use.\n",
    "\n",
    "Note that the equals sign is directional, like an arrow, telling the computer to give a certain value to a certain label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'a' is being given the value 2; 'b' is given 5\n",
    "\n",
    "a = 2\n",
    "b = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's perform an operation on the variables\n",
    "\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables can have many different kinds of names\n",
    "\n",
    "this_number = 2\n",
    "b/this_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings\n",
    "\n",
    "In Python, human language text gets represented as a <i>string</i>. These contain sequential sets of characters and they are offset by quotation marks, either double (\") or single (').\n",
    "\n",
    "We will explore different kinds of operations in Python that are specific to human language objects, but it is useful to start by trying to see them as the computer does, as numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The iconic string\n",
    "\n",
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign these strings to variables\n",
    "\n",
    "a = \"Hello\"\n",
    "b = 'World'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try out arithmetic operations.\n",
    "# When we add strings we call it 'concatenation'\n",
    "\n",
    "print(a+b)\n",
    "print(a*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unlike a number that consists of a single value, a string is an ordered\n",
    "# sequence of characters. We can find out the length of that sequence.\n",
    "\n",
    "len(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. How long is the string below?\n",
    "\n",
    "this_string = \"It was the best of times; it was the worst of times.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists\n",
    "\n",
    "The <i>numbers</i> and <i>strings</i> we have just looked at are the two basic data types that we will focus our attention on in this workshop. (In a few days, we will look at a third data type, <i>boolean</i>, which consists of just True/False values.) When we are working with just a few numbers or strings, it is easy to keep track of them, but as we collect more we will want a system to organize them.\n",
    "\n",
    "One such organizational system is a <i>list</i>. This contains values (regardless of type) in order, and we can perform operations on it very similarly to the way we did with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A list in which each element is a string\n",
    "\n",
    "['Call', 'me', 'Ishmael']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's assign a couple lists to variables\n",
    "\n",
    "list1 = ['Call', 'me', 'Ishmael']\n",
    "list2 = ['In', 'the', 'beginning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Q. Predict what will happen when we perform the following operations\n",
    "\n",
    "print(list1+list2)\n",
    "print(list1*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Like a string, we can find out the length of a list\n",
    "\n",
    "len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sometimes we just want a single value from the list at a time\n",
    "\n",
    "print(list1[0])\n",
    "print(list1[1])\n",
    "print(list1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or maybe we want the first few\n",
    "\n",
    "print(list1[0:2])\n",
    "print(list1[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Concatenate 'list1' and 'list2' into a single list.\n",
    "##     Retrieve the third element from the combined list.\n",
    "##     Retrieve the fourth through sixth elements from the combined list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Methods\n",
    "\n",
    "The creators of Python recognize that human language has many important yet idiosyncratic features, so they have tried to make it easy for us to identify and manipulate them. For example, in the demonstration at the very beginning of the workshop, we referred to the idea of the suffix: the final letters of a word tell us something about its grammatical role and potentially semantics.\n",
    "\n",
    "We can analyze or manipulate certain features of a string using its <i>methods</i>. These are basically internal functions that every string automatically gets assigned. Note that even though the method may transform the string at hand, they don't change it permanently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's assign a variable to perform methods upon\n",
    "\n",
    "greeting = \"Hello, World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We saw the 'endswith' method at the very beginning\n",
    "# Note the type of output that gets printed\n",
    "\n",
    "greeting.startswith('H'), greeting.endswith('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can check whether the string is a letter or a number\n",
    "\n",
    "# When there are multiple characters, it checks whether *all*\n",
    "# of the characters belong to that category\n",
    "\n",
    "greeting.isalpha(), greeting.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similarly, we can check whether the string is lower or upper case\n",
    "\n",
    "greeting.islower(), greeting.isupper(), greeting.istitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sometimes we want not just to check, but to change the string\n",
    "\n",
    "greeting.lower(), greeting.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The case of the string hasn't changed!\n",
    "\n",
    "greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But if we want to permanently make it lower case we re-assign it\n",
    "\n",
    "greeting = greeting.lower()\n",
    "\n",
    "greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oh hey. And strings are kind of like lists, so we can slice them similarly\n",
    "\n",
    "greeting[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Strings may be like lists of characters, but as humans we treat them as\n",
    "# lists of words. We can perform that conversion for the computer.\n",
    "\n",
    "greeting.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Return the second through eighth characters in 'greeting'\n",
    "\n",
    "## EX. Split the string below into a list of words and assign this to a new variable\n",
    "## Note: A slash at the end of a line allows a string to continue unbroken onto the next\n",
    "\n",
    "## Challenge: Return the characters from the first half of 'greeting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_string = \"It, is a truth universally acknowledged, that a single \\\n",
    "man in possession of a good fortune must be in want of a wife.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## List Comprehension\n",
    "\n",
    "List comprehensions are a fairly advanced programming technique that we will spend more time talking about tomorrow. For now, you can think of them as list filters. Often, we don't need every value in a list, just a few that fulfill certain criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 'list1' had contained three words, two of which were in title case.\n",
    "# We can automatically return those words using a list comprehension\n",
    "\n",
    "[word for word in list1 if word.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Or we can include all the words in the list but just take their first letters\n",
    "\n",
    "[word[0] for word in list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Using the list of words you produced by splitting 'new_string', create\n",
    "##     a new list that contains only the words whose last letter is \"e\" \n",
    "\n",
    "## EX. Create a new list that contains the first letter of each word.\n",
    "\n",
    "## EX. Create a new list that contains only words longer than two letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLTK\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is a suite of Natural Language Processing tools that are designed to be easy to use while achieving near state-of-the-art performance. These will allow us to do things like tag parts of speech to the words in our strings, identify synonyms and other semantic relationships, and perform useful statistics on our collections of words.\n",
    "\n",
    "We will look at NLTK's most essential functions in the next few days in order to start building our tools from the ground up. Today, however, we will focus our attention on a few popular, higher order functions that NLTK makes available, including producing concordances and counting word frequencies.\n",
    "\n",
    "To be more precise, NLTK is a <i>package</i>, which expands the functionality of Python by making new functions, methods, and data available. In order to access a package, we must first download it to our computers (which you have already done if you installed Anaconda) and then activate it by <i>importing</i> the package into our script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essential NLTK functions\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>Processing raw text</td>\n",
    "        <td>nltk.tokenize, nltk.stem</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Part of Speech and Grammar</td>\n",
    "        <td>nltk.tag, nltk.chunk, nltk.parse</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Semantic Meaning</td>\n",
    "        <td>nltk.stem.wordnet</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Statistical measures</td>\n",
    "        <td>nltk.metrics, nltk.probability, nltk.collocations</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example NLTK corpora of humanistic interest\n",
    "\n",
    "<table align='left'>\n",
    "    <tr>\n",
    "        <td>Project Gutenberg: Lit</td>\n",
    "        <td>nltk.corpus.gutenberg\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Webpages, chat</td>\n",
    "        <td>nltk.corpus.webtext\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Twitter</td>\n",
    "        <td>nltk.corpus.twitter\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Brown Corpus</td>\n",
    "        <td>nltk.corpus.brown\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Reuters: News</td>\n",
    "        <td>nltk.corpus.reuters\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Inaugural Addr. (US)</td>\n",
    "        <td>nltk.corpus.inaugural</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read Mody Dick from a plaintext file in the current folder\n",
    "\n",
    "moby_string = open('Melville - Moby Dick.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the string!\n",
    "\n",
    "moby_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the string into a list of words\n",
    "\n",
    "moby_list = moby_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the beginning of the list\n",
    "\n",
    "moby_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import NLTK!\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use NLTK's 'Text' function to give 'moby_list' some new methods\n",
    "\n",
    "moby_text = nltk.Text(moby_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moby_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Has the familiar list slicing method\n",
    "\n",
    "moby_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Has the same length as our old 'moby_list'\n",
    "\n",
    "len(moby_list), len(moby_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But it's now very easy to produce a concordance\n",
    "\n",
    "moby_text.concordance('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can also find words that appear in similar contexts\n",
    "\n",
    "moby_text.similar('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And we can find out what those shared contexts are\n",
    "\n",
    "moby_text.common_contexts([\"whale\", 'ship'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a report of word frequencies in the text\n",
    "\n",
    "nltk.FreqDist(moby_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign these to a variable\n",
    "\n",
    "fdist = nltk.FreqDist(moby_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ten most frequent words in Moby Dick\n",
    "\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequency of 'whale' in Moby Dick\n",
    "\n",
    "fdist['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Often we want to normalize the frequency as a proportion of all words in the text\n",
    "\n",
    "fdist.freq('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's get a simple list of unique words in Moby Dick\n",
    "\n",
    "fdist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. A common measure of lexical diversity for a given text is its Type-Token Ratio:\n",
    "##     the average number of times each word in a text gets used.\n",
    "##     Calculate the Type-Token Ratio for Moby Dick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a special code for Jupyter, so that it produces graphs inside the notebook\n",
    "\n",
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's graph the top 50 words in Moby Dick by descending frequency\n",
    "\n",
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternately, we can plot the words cumulatively so we can\n",
    "# see how much of the text is accounted\n",
    "\n",
    "fdist.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EX. Transform the script below into a list of words, then plot their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "script = \"Man: Well, what've you got? Waitress: Well, there's egg and bacon; egg sausage and bacon; \\\n",
    "egg and spam; egg bacon and spam; egg bacon sausage and spam; spam bacon sausage and spam; \\\n",
    "spam egg spam spam bacon and spam; spam sausage spam spam bacon spam tomato and spam; \\\n",
    "spam spam spam egg and spam; spam spam spam spam spam spam baked beans spam spam spam; \\\n",
    "...or Lobster Thermidor au Crevette with a Mornay sauce served in a Provencale manner with shallots \\\n",
    "and aubergines garnished with truffle pate, brandy and with a fried egg on top and spam.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing Alliterative Frequency\n",
    "\n",
    "Thornbury's argument for dual authorship of \"Christ and Satan\" had relied upon the fact that there was a large, anomolous divide between fitts with alliterative distributions above and below the average frequency for the text. Using general Python techniques and specific tools from NLTK that we have seen, we can now approximate that average alliterative distribution curve for \"Christ and Satan\" by collecting the first letter of each word used in the poem.\n",
    "\n",
    "The cell below will read in the text of \"Christ and Satan\" from a file that resides on your hard-drive, and assign the text as a string to the variable 'cs_text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Plot a frequency graph, not for its words, but for the first letter of each word in \"Christ and Satan.\"\n",
    "## Hint: You will need to use a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cs_text = open('christ-and-satan.txt').read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Humanistic Research Question...</h1>\n",
    "<img src='Long, So 263, Fig 8.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literary Patterns (Probably)\n",
    "<ul><li>Preview</li>\n",
    "<li>Review</li>\n",
    "<li>Import Corpus</li>\n",
    "<li>Pre-Processing</li>\n",
    "<ul><li>Tokenize</li>\n",
    "<li>Stop Words</li>\n",
    "<li>Lemmatize</li>\n",
    "<li>Most Frequent Terms</li></ul>\n",
    "<li>Classification</li>\n",
    "<ul><li>Featurize</li>\n",
    "<li>Training</li>\n",
    "<li>Prediction</li>\n",
    "<li>Extra: Cross-Validation</li></ul>\n",
    "<li>Literary Distinction</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt','wordnet', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get texts of interest that belong to identifiably different categories\n",
    "\n",
    "unladen_swallow = 'high air-speed velocity'\n",
    "swallow_grasping_coconut = 'low air-speed velocity'\n",
    "\n",
    "# Transform them into the format NLTK expects\n",
    "\n",
    "unladen_features_tagged = ({'high':True, 'air-speed': True, 'velocity': True}, 'unladen')\n",
    "coconut_features_tagged = ({'low':True, 'air-speed': True, 'velocity': True}, 'coconut')\n",
    "\n",
    "# Train a classifier to learn distinguishing features\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train([unladen_features_tagged, coconut_features_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's a simple question of weight ratios!\n",
    "# A five ounce bird could not carry a one pound coconut. \n",
    "\n",
    "unknown_swallow = \"high velocity\"\n",
    "unknown_features = { 'high': True, 'velocity':True}\n",
    "\n",
    "classifier.classify(unknown_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Moby Dick\n",
    "moby_string = open('Melville - Moby Dick.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the text\n",
    "moby_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the text lower case\n",
    "moby_lower = moby_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize Moby Dick\n",
    "from nltk import word_tokenize\n",
    "moby_tokens = word_tokenize(moby_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out the tokens\n",
    "moby_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just how long is Moby Dick anyway?\n",
    "len(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dictionary that counts token frequencies\n",
    "from collections import Counter\n",
    "moby_dict = Counter(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionaries pair keys with values\n",
    "moby_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Report the ten most common tokens in the novel\n",
    "moby_dict.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the frequency of a specific word\n",
    "moby_dict['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list comprehension, including an 'if' statement\n",
    "just_whales = [token for token in moby_tokens if token=='whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hast seen the White Whale?\n",
    "just_whales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating System Interface!\n",
    "\n",
    "Up to this point, we have only worked with one text at a time. It is simple enough to read in a single plaintext file, but we often find ourselves with many files residing in a folder on our hard drive. In order to access these, we will have to instruct Python to navigate our computer and access the files sequentially.\n",
    "\n",
    "Even though it sounds banal, this is the moment your computer ceases to be an appliance and transforms into a tool. The <i>os</i> package allows Python to speak with the rest of your computer's systems and file storage. You now have access to any file on your computer and can manipulate them using the code you have learned so far. With great power comes great responsibility!\n",
    "\n",
    "For now, we will look at just one function from <i>os</i> that will return a list of the files in a given directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Report the files in the current folder\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Follow one of the reported folders\n",
    "os.listdir('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And follow deeper\n",
    "os.listdir('movie_reviews/negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign that list to a variable\n",
    "negative_files = os.listdir('movie_reviews/negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect first element in the list\n",
    "negative_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EX. How many reviews are there in the 'positive' folder?\n",
    "##     How many in the 'negative' one?\n",
    "\n",
    "negative_path = \"movie_reviews/negative/\"\n",
    "positive_path = \"movie_reviews/positive/\"\n",
    "\n",
    "## CHALLENGE: Find a list of files and folders on your desktop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "\n",
    "Although Long and So's study of modernist haiku motivates this lesson, a substantial portion of their corpus remains under copyright so they have not made it available publicly. Instead we will apply their methods to a toy corpus distributed with NLTK: movie reviews. These have been divided into positive and negative categories, with one thousand of each.\n",
    "\n",
    "In essence, our task will be to learn the vocabulary of positive and negative evaluation, rather than the poetic genre of haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the first file from 'negative_files'\n",
    "open('movie_reviews/negative/cv000_29416.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When opening others, filenames change but the path doesn't!\n",
    "negative_path = \"movie_reviews/negative/\"\n",
    "open(negative_path+'cv000_29416.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all files and assign to a variable\n",
    "negative_reviews = [open(negative_path+name,'r').read() for name in negative_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: If you are using OSX, your operating system may sometimes\n",
    "# include hidden files in your folders that confuse Python.\n",
    "\n",
    "# If you get an error while running the above line, try including an 'if' condition\n",
    "# in your list comprehension to prevent Python from tripping over these.\n",
    "\n",
    "# For example:\n",
    "negative_reviews = [open(negative_path+name,'r').read() for name in negative_files if name[-4:]=='.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "negative_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat process for positive reviews\n",
    "\n",
    "positive_path = 'movie_reviews/positive/'\n",
    "positive_files = os.listdir(positive_path)\n",
    "positive_reviews = [open(positive_path+name,'r').read() for name in positive_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect first element in list\n",
    "positive_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. How long is the list of positive movie reviews? Negative reviews?\n",
    "##     Do these match the number of files you had observed in the folders earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-Processing\n",
    "\n",
    "The pre-processing phase of our workflow transforms the strings that Python has read from the plaintext files into useful sets of features. Not only will we tokenize the texts, as we have previously, but we will perform three further steps described by Long and So: <i>stop word</i> removal, <i>lemmatization</i> of nouns, and low-frequency word removal.\n",
    "\n",
    "Although pre-processing feels like a nitty-gritty task, it is important to recognize that how we pre-process our texts depends on the questions we are trying to answer. Not every project lemmatizes or stems its vocabulary. Perhaps we can imagine research questions in which the grammmatical functions indicated by word endings might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize first negative review\n",
    "\n",
    "word_tokenize(negative_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize our sets of reviews; tokens remain grouped by review\n",
    "\n",
    "negative_tokenized = [word_tokenize(review) for review in negative_reviews]\n",
    "positive_tokenized = [word_tokenize(review) for review in positive_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q.  The texts in the movie review corpus are already in lower case, however many\n",
    "##     texts found in the wild are not. How would you change the code in the cell above\n",
    "##     to produce a corpus of tokenized and lower-case text?\n",
    "\n",
    "## EX. How many tokens are there in the first negative movie review? positive?\n",
    "\n",
    "## CHALLENGE: How many tokens are there on average in each negative movie review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "Stop words, sometimes refered to as <i>function words</i>, include articles, prepositions, pronouns, and conjunctions among others. Although their frequencies encode information about textual features like authorship, they do not convey semantic meanings and are often removed before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import our list of stop words\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull up NLTK's list of English-language stop words\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many stop words are in the list?\n",
    "\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK has stopwords for many Western languages\n",
    "\n",
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = ['what', 'is', 'the', 'air-speed', 'velocity', 'of', 'an', 'unladen', 'swallow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from tokenized review\n",
    "\n",
    "for word in tokenized_sentence:\n",
    "    if word not in stopwords.words('english'):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a list comprehension\n",
    "\n",
    "[word for word in tokenized_sentence if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# But what if we have more than one review at a time?\n",
    "\n",
    "two_sentences = [['what', 'is', 'the', 'air-speed', 'velocity', 'of', 'an', 'unladen', 'swallow'],\\\n",
    "               ['what', 'do', 'you', 'mean', 'african', 'or', 'european']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a nested for-loop!\n",
    "\n",
    "for sentence in two_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in stopwords.words('english'):\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a nested list comprehension: produces a list of lists!\n",
    "\n",
    "[[word for word in sentence if word not in stopwords.words('english')] for sentence in two_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The example sentences were short so it's hard to tell, but looking up\n",
    "# whether a word is 'in' a list takes a pretty long time\n",
    "\n",
    "# We can improve the speed of our task by converting the list to a set!\n",
    "\n",
    "stopword_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "stopword_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And we can remove the stop words from our movie reviews\n",
    "\n",
    "negative_no_stops = [[word for word in review if word not in stopword_set] for review in negative_tokenized]\n",
    "positive_no_stops = [[word for word in review if word not in stopword_set] for review in positive_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q.  Stop words are typically the most frequent words in a language, yet do not convey semantic meaning.\n",
    "##     Does this make sense based on the words in NLTK's list of English stop words?\n",
    "##     What about other languages with which you are familar?\n",
    "\n",
    "stopword_languages = ['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian',\\\n",
    "                      'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']\n",
    "\n",
    "## EX. How many tokens did we remove from the first negative review in total? What percent were removed?\n",
    "\n",
    "## EX. Try rewriting the list comprehensions in the above cell as plain for-loops.\n",
    "\n",
    "## CHALLENGE.  Stop words are often instrumental in language detection for unknown texts.\n",
    "##             How might you write a program to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize\n",
    "\n",
    "Lemmatization transforms an inflected word into its root. For nouns, this often converts plural form to singular. For verbs, this often produces the infinitive. Long and So lemmatize only nouns for their project -- which is just as well because NLTK's lemmatization function assumes that all words are nouns by default and leaves most non-nouns untouched!\n",
    "\n",
    "<h3>WordNet Abbreviations</h3>\n",
    "<table align='left'>\n",
    "<tr><td>Noun</td><td>'n'</td><td>wordnet.NOUN</td></tr>\n",
    "<tr><td>Verb</td><td>'v'</td><td>wordnet.VERB</td></tr>\n",
    "<tr><td>Adjective</td><td>'a'</td><td>wordnet.ADJ</td></tr>\n",
    "<tr><td>Adverb</td><td>'r'</td><td>wordnet.ADV</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import NLTK's lemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer and assign it to a variable\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The lemmatization is called as a method: .lemmatize()\n",
    "\n",
    "wnl.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Doesn't seem to work properly on 'running' because 'wnl' assumes it is seeing a noun\n",
    "\n",
    "wnl.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fortunately we can pass in a label for a different part of speech\n",
    "# Perhaps one might use this along with a POS tagger!\n",
    "\n",
    "wnl.lemmatize('running', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A new list of tokens\n",
    "\n",
    "famous_sketch = ['ministry','of', 'silly', 'walks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a for-loop to lemmatize each word sequentially\n",
    "\n",
    "for word in famous_sketch:\n",
    "    print(wnl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a list comprehension!\n",
    "\n",
    "[wnl.lemmatize(word) for word in famous_sketch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, two lists of tokens\n",
    "\n",
    "two_sketches = [['ministry','of', 'silly', 'walks'],['musical','mice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As a nested list comprehension!\n",
    "\n",
    "[[wnl.lemmatize(word) for word in sketch] for sketch in two_sketches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's lemmatize the nouns in our movie reviews\n",
    "\n",
    "negative_lemmatized = [[wnl.lemmatize(word) for word in review] for review in negative_no_stops]\n",
    "positive_lemmatized = [[wnl.lemmatize(word) for word in review] for review in positive_no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Lemmatize the list of plural nouns below\n",
    "\n",
    "plural_nouns = ['parrots', 'witches', 'volcanoes', 'soliloquies', 'cherries', 'addenda', \\\n",
    "                'baths', 'knives', 'oxen', 'lice', 'brethren', 'alumni', 'alumnae', 'matrices']\n",
    "\n",
    "## CHALLENGE: Use the part-of-speech tagger we looked at yesterday and include\n",
    "##            a POS argument while lemmatizing the following sentence.\n",
    "\n",
    "brave_sir_robin = \"When danger reared its ugly head, he bravely turned his tail and fled!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Document Frequency\n",
    "\n",
    "Intuitively, not all words in the corpus will convey the same amount of information about whether a movie review is positive or negative or whether a poem is a haiku. At the extreme, if a word appears in just a single text out of thousands, it doesn't tell us much either way about whether that word is associated with a category. By removing infrequent terms from our model, we can also save computational time.\n",
    "\n",
    "When we measure document frequency, we do not need to know how many times a token appears in a text. We simply need to know which tokens appear in each. Python has an easy, built-in data type that that tells us the unique elements appearing in a list: <i>set</i>. As a data-type, a set is like a list but it does not retain information about the order of elements. Also, it is very effient when we want to check 'if' a particular element is contained 'in' a group of words.\n",
    "\n",
    "In order to count the document frequency for each word in the corpus, we will produce a set of unique words for each text. Then we will pull out each word from each set and put these into a single list. Words belonging to multiple sets will appear multiple times; words belonging to just a single set will appear only once. Finally, we will use the <i>Counter</i> to tally how many times each word appears in the term-document frequency list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set: Function/Data-Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's a text that reuses many of its words\n",
    "\n",
    "gertrude_stein = ['rose','is', 'a', 'rose', 'is', 'a', 'rose', 'is', 'a', 'rose', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return a 'set' of the unique tokens in the text\n",
    "# A set is like a list but does not retain information about order\n",
    "\n",
    "set(gertrude_stein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The size of the set is much smaller than the text itself\n",
    "\n",
    "len(gertrude_stein), len(set(gertrude_stein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produce a set of unique tokens contained in each review\n",
    "\n",
    "negative_sets = [set(review) for review in negative_lemmatized]\n",
    "positive_sets = [set(review) for review in positive_lemmatized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want frequencies for the whole corpus, so we'll put our sets of words together now\n",
    "\n",
    "all_sets = negative_sets + positive_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that we got them all\n",
    "\n",
    "len(negative_sets), len(positive_sets), len(all_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And we'll pull out the words from each review set\n",
    "# This produces a list in which each token appears as many times as the number documents to which it belongs\n",
    "\n",
    "term_document_frequency_list = [word for review in all_sets for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want to count the number of times each token appears, so we'll use 'Counter'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tdf_counts = Counter(term_document_frequency_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "tdf_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's refresh ourselves on Counter's methods\n",
    "\n",
    "tdf_counts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many reviews refer to the larger film industry?\n",
    "\n",
    "tdf_counts['hollywood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produce a list of words whose tdf-count is greater than 1\n",
    "\n",
    "more_than_once = [key for key in tdf_counts.keys() if tdf_counts[key]>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "more_than_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just how many words did we remove from our vocabulary?\n",
    "\n",
    "len(more_than_once), len(tdf_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can go back through our movie reviews and remove\n",
    "# any words that were not included in 'more_than_once'\n",
    "\n",
    "# As mentioned above, it is much more efficient to perform that task using a 'set'\n",
    "\n",
    "more_than_once_set = set(more_than_once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_min_df = [[word for word in review if word in more_than_once_set] for review in negative_sets]\n",
    "positive_min_df = [[word for word in review if word in more_than_once_set] for review in positive_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "negative_min_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX.  Get a list of the 500 words with the highest document frequencies.\n",
    "##      Only words should be contained in the list.\n",
    "\n",
    "## CHALLENGE: Long and So remove low-frequency words based on document frequency.\n",
    "##            Another popular method is to retain only high-frequency\n",
    "##            words based on the raw number of times they appear in the corpus\n",
    "##            (i.e. the sum of their counts in all texts).\n",
    "\n",
    "##            Use this method to find the 500 most common terms in the poetry corpus.\n",
    "##            Does this match with the list in the previous exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize\n",
    "\n",
    "For humans, reading a string of text is a relatively easy task, but for the computer to learn about language, text has to be represented in very particular ways. We refer to this as <i>featurization</i>: the transformation of a text into a quantitative feature set.\n",
    "\n",
    "In order for the NLTK classifier to work, we have to represent each text as a set of True/False values: Is a given word from our high-frequency vocabulary present in this review? More specifically, these values will be contained in a <i>dictionary</i>, where each key is a vocabulary word and its value is whether or not it is present. In fact, we need only to include terms that are present, so all of our values will be True.\n",
    "\n",
    "Once we have processed each text according to this rubric, we will then attach a label for the text's category ('reviewed'/'random). The classifier will use this to identify which features are associated with each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's revisit some earlier tokens\n",
    "\n",
    "unladen_tokens = ['high','air-speed','velocity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to represent our tokens to the classifier, we need to\n",
    "# associate them with a 'True' value in a dictionary\n",
    "\n",
    "# Sure looks like a list comprehension!\n",
    "\n",
    "{token:True for token in unladen_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In general, we are not limited to True/False values to our dictionary entries\n",
    "# For example:\n",
    "\n",
    "{token:len(token) for token in unladen_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turn our reviews into dictionaries that indicate whether a word is present\n",
    "\n",
    "negative_featurized = [{word:True for word in review} for review in negative_min_df]\n",
    "positive_featurized = [{word:True for word in review} for review in positive_min_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect first review\n",
    "\n",
    "negative_featurized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Attach a label to each review\n",
    "negative_tagged = [(review,'negative') for review in negative_featurized]\n",
    "positive_tagged = [(review,'positive') for review in positive_featurized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "negative_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine these lists of featurized, tagged reviews\n",
    "all_tagged = negative_tagged + positive_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOTE. We'll spend more time with dictionaries tomorrow, so let's hand wave\n",
    "##       it as a formatting step for now and move on to classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "We have selected an algorithm that specifically relies on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes' Theorem</a> to model relationships between textual features and categories in our corpus of movie reviews. (See link for more information about the method and its assumptions.)\n",
    "\n",
    "Two ways that we learn about the model are its feature weights and predictions on new texts. The algorithm can explicity report to us which direction each word leans category-wise and how strongly. Based on those weights, it makes further predictions about the valences previously unseen movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the classifier and assign it to a variable\n",
    "\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(all_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Report feature information\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Roger Ebert's reviews of a couple family films\n",
    "\n",
    "# \"This movie made my heart glad. It is filled with innocence, hope, and good cheer.\"\n",
    "#    -Roger Ebert, on ET\n",
    "\n",
    "positive_ET = {'best': True, 'baffled': True, 'space': True, 'relationship': True, 'people': True, 'little': True,\\\n",
    "            'friendship': True, 'love': True, 'story': True, 'becomes': True,'hear': True, 'outer': True,\\\n",
    "            'boy': True, 'friend': True, 'tells': True, 'creature': True, 'described': True}\n",
    "\n",
    "# \"I hated this movie. Hated, hated, hated, hated, hated this movie. Hated it.\n",
    "#  Hated every simpering stupid vacant audience-insulting moment of it.\"\n",
    "#     -Roget Ebert, on \"North\"\n",
    "\n",
    "negative_north = {'simpering': True, 'belief': True, 'every': True, 'thought': True, 'implied': True,\\\n",
    "                  'entertained': True, 'insulting': True, 'vacant': True, 'sensibility': True, 'stupid': True,\\\n",
    "                  'insult': True, 'audience': True, 'anyone': True, 'movie': True, 'hated': True, 'moment': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What does the classifier think?\n",
    "\n",
    "classifier.classify(positive_ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.classify(negative_north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for a list of reviews\n",
    "\n",
    "review_list = [positive_ET, negative_north]\n",
    "classifier.classify_many(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Although our classification is binary, Bayes theorem assigns\n",
    "# a probability of membership in either category\n",
    "\n",
    "# Just how confident is our classifier of its predictions?\n",
    "\n",
    "classifier.prob_classify(positive_ET).prob('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.prob_classify(negative_north).prob('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. There are two movie reviews that the clasifier has not yet seen in the folder below.\n",
    "##     Use the classifier to predict whether they are positive or negative.\n",
    "\n",
    "## Q.  What kinds of patterns do you notice among the 'most informative features'\n",
    "##     in the movie review corpus. Where are critics focusing their attention?\n",
    "##     Try looking at the top several hundred most informative words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Cross-Validation\n",
    "\n",
    "Just how good is our classifier? We can evaluate it by randomly selecting reviews from each category and setting them aside before training. We then see how well the classifier predicts their (known) categories.\n",
    "\n",
    "Remember that if the classifier is trying to predict membership for just two categories, we would expect it to be correct about 50% of the time based on random chance. As a rule of thumb, if this kind of classifier has 65% accuracy or better under cross-validation, it has often identified a meaningful pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomize our list of movie reviews (in place)\n",
    "\n",
    "import numpy\n",
    "numpy.random.shuffle(all_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll train our classifier on the first 90% of reviews\n",
    "# and validate using the last 10%\n",
    "\n",
    "training_set = all_tagged[:-72]\n",
    "validation_set = all_tagged[-72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train, validate\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "nltk.classify.accuracy(classifier, validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. In fact, this is not the best implementation of cross-validation, since we used\n",
    "##     the entire corpus to identify words that appear more than once. In effect, we have\n",
    "##     passed information from our validation set into the classifier we wish to test.\n",
    "\n",
    "##     Repeat the processing of the corpus based only on the training set and use this to\n",
    "##     make predictions about the validation set. How much does the classifier's accuracy change? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
